{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JpR432XaXmR"
      },
      "source": [
        "We are interested in the [STEAD](https://github.com/smousavi05/STEAD) dataset.\n",
        "\n",
        "The STEAD format refers to the STanford EArthquake Dataset, which is a large-scale dataset of seismic waveforms designed for machine learning applications. It contains millions of labeled seismic waveforms, including earthquakes and noise records, with metadata such as event magnitude, depth, station location, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can download each chunk from the [Github](https://github.com/smousavi05/STEAD?tab=readme-ov-file) page of the STEAD dataset\n",
        "\n",
        "Each chunk consists of an HDF5 file (containing waveform data) and a CSV file (containing metadata).\n",
        "\n",
        "* Note1: some of the unzipper programs for Windows and Linux operating systems have size limits. Try '7Zip' software if had problems unzipping the files.\n",
        "\n",
        "* Note2: all the metadata are also available in the hdf5 file (as attributes associated with each waveform). But the CSV file can be used to easily select a specific part of the dataset and only read associated waveforms from the hdf5 file for efficiency.\n",
        "\n",
        "* Note3: For some of the noise data waveforms are identical for 3 components. These are related to single-channel stations where we duplicated the vertical channel for horizontal ones. However, these makeup to less than 4 % of noise data. For the rest, noise is different for each channel.\n",
        "\n",
        "Before proceeding is useful to understand what is the structure of a hdf5 file... the basic components are:\n",
        "\n",
        "1. 📂 **Groups (Like Folders in a File System)**: Groups are containers that organize datasets and other groups (like directories in a filesystem). The root group ` / ` is the top-level container in the HDF5 file.\n",
        "Example:\n",
        "```\n",
        "/\n",
        "├── metadata\n",
        "├── data\n",
        "│   ├── event1\n",
        "│   ├── event2\n",
        "│   ├── event3\n",
        "```\n",
        "\n",
        "2. 📄 **Datasets (Like Files in a Folder)**: Datasets contain the actual numerical or text data (like files in a folder). Datasets can be multi-dimensional (like NumPy arrays).\n",
        "Example:\n",
        "```\n",
        "data/event1  →  [1000x3] array (1000 samples, 3 components)\n",
        "```\n",
        "\n",
        "3. 📝 **Attributes (Metadata Associated with Groups or Datasets)**: Attributes store metadata about groups or datasets (like file properties).\n",
        "Example:\n",
        "```\n",
        "data/event1.attrs\n",
        "├── p_arrival_sample: 230\n",
        "├── s_arrival_sample: 450\n",
        "├── coda_end_sample: 800\n",
        "├── station_name: ABC\n",
        "├── magnitude: 3.5\n",
        "```\n",
        "Attributes cannot be datasets but are small pieces of metadata (e.g., timestamps, location, experiment details)\n",
        "\n",
        "The structure of a chunk of the STEAD dataset looks like \n",
        "```\n",
        "/\n",
        "├── data (group)\n",
        "│   ├── event1 (dataset, 1000x3 array)\n",
        "│   │   ├── p_arrival_sample: 230\n",
        "│   │   ├── s_arrival_sample: 450\n",
        "│   │   ├── coda_end_sample: 800\n",
        "│   │   ├── source_magnitude: 3.5\n",
        "│   │   ├── source_distance_km: 15.2\n",
        "│   │   ├── ...\n",
        "│   │\n",
        "│   ├── event2 (dataset, 1200x3 array)\n",
        "│   │   ├── p_arrival_sample: 250\n",
        "│   │   ├── s_arrival_sample: 470\n",
        "│   │   ├── ...\n",
        "│   │\n",
        "│   ├── ...\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any further understanding of the dataset please visit the official GitHub page and the [STanford EArthquake Dataset (STEAD): A Global Data Set of Seismic Signals for AI](https://www.researchgate.net/publication/336598670_STanford_EArthquake_Dataset_STEAD_A_Global_Data_Set_of_Seismic_Signals_for_AI) article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m21qgQ8JrLRl"
      },
      "source": [
        "# hdf5 to Torch_ensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "collapsed": true,
        "id": "aoBKbvlX0A0F",
        "outputId": "0feccbaf-b6bf-4a0d-ce5d-973fdf8098a0"
      },
      "outputs": [],
      "source": [
        "! pip install obspy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkJy-qutrUl_",
        "outputId": "27fea303-226c-4aea-93f1-3e4bf76f0089"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-052e8900cc58>:13: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(csv_file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total events in csv file: 200000\n",
            "198724\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import h5py\n",
        "import numpy as np\n",
        "import obspy\n",
        "import torch\n",
        "from obspy import UTCDateTime\n",
        "from obspy.clients.fdsn import Client\n",
        "\n",
        "file_name = \"path/to/STEAD_data/chunk2.hdf5\"\n",
        "csv_file = \"path/to/STEAD_data/chunk2/chunk2.csv\"\n",
        "\n",
        "# reading the csv file into a dataframe:\n",
        "df = pd.read_csv(csv_file)\n",
        "print(f'total events in csv file: {len(df)}')\n",
        "df = df[~df.network_code.isin(['IV', 'HA', 'KO', 'HP', 'FR', 'S', 'TU'])]\n",
        "\n",
        "# making a list of trace names for the selected data\n",
        "ev_list = df['trace_name'].to_list()\n",
        "print(len(ev_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EbFEB0qn9yf"
      },
      "outputs": [],
      "source": [
        "#custom to deal with 12Z casescases\n",
        "\n",
        "def make_stream(dataset):\n",
        "    '''\n",
        "    input: hdf5 dataset\n",
        "    output: obspy stream\n",
        "\n",
        "    '''\n",
        "    data = np.array(dataset)\n",
        "\n",
        "    tr_E = obspy.Trace(data=data[:, 0])\n",
        "    tr_E.stats.starttime = UTCDateTime(dataset.attrs['trace_start_time'])\n",
        "    tr_E.stats.delta = 0.01\n",
        "    if(dataset.attrs['network_code'] == 'PB'):\n",
        "        tr_E.stats.channel = dataset.attrs['receiver_type']+'1'\n",
        "    else:\n",
        "        tr_E.stats.channel = dataset.attrs['receiver_type']+'E'\n",
        "    tr_E.stats.station = dataset.attrs['receiver_code']\n",
        "    tr_E.stats.network = dataset.attrs['network_code']\n",
        "    if(dataset.attrs['network_code'] == 'GM'):\n",
        "        tr_E.stats.location = '01'\n",
        "    elif(dataset.attrs['network_code'] in ['II', 'US', 'NM', 'ET']):\n",
        "        tr_E.stats.location = '00'\n",
        "\n",
        "    tr_N = obspy.Trace(data=data[:, 1])\n",
        "    tr_N.stats.starttime = UTCDateTime(dataset.attrs['trace_start_time'])\n",
        "    tr_N.stats.delta = 0.01\n",
        "    if(dataset.attrs['network_code'] == 'PB'):\n",
        "        tr_N.stats.channel = dataset.attrs['receiver_type']+'2'\n",
        "    else:\n",
        "        tr_N.stats.channel = dataset.attrs['receiver_type']+'N'\n",
        "    tr_N.stats.station = dataset.attrs['receiver_code']\n",
        "    tr_N.stats.network = dataset.attrs['network_code']\n",
        "    if(dataset.attrs['network_code'] == 'GM'):\n",
        "        tr_N.stats.location = '01'\n",
        "    elif(dataset.attrs['network_code'] in ['II', 'US', 'NM', 'ET']):\n",
        "        tr_N.stats.location = '00'\n",
        "\n",
        "    tr_Z = obspy.Trace(data=data[:, 2])\n",
        "    tr_Z.stats.starttime = UTCDateTime(dataset.attrs['trace_start_time'])\n",
        "    tr_Z.stats.delta = 0.01\n",
        "    tr_Z.stats.channel = dataset.attrs['receiver_type']+'Z'\n",
        "    tr_Z.stats.station = dataset.attrs['receiver_code']\n",
        "    tr_Z.stats.network = dataset.attrs['network_code']\n",
        "    if(dataset.attrs['network_code'] == 'GM'):\n",
        "        tr_Z.stats.location = '01'\n",
        "    elif(dataset.attrs['network_code'] in ['II', 'US', 'NM', 'ET']):\n",
        "        tr_Z.stats.location = '00'\n",
        "\n",
        "    stream = obspy.Stream([tr_E, tr_N, tr_Z])\n",
        "\n",
        "    return stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGpphsrMpVQs"
      },
      "outputs": [],
      "source": [
        "client = Client(\"IRIS\")\n",
        "\n",
        "chunk = \"chunk2\"\n",
        "\n",
        "waveform_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_4g0eLrsoyc"
      },
      "outputs": [],
      "source": [
        "# retrieving selected waveforms from the hdf5 file:\n",
        "dtfl = h5py.File(file_name, 'r')\n",
        "for c, evi in enumerate(ev_list):\n",
        "\n",
        "    if c%100 == 0:\n",
        "      print(f'{c}')\n",
        "\n",
        "    dataset = dtfl.get('data/'+str(evi))\n",
        "\n",
        "    # waveforms, 3 channels: first row: E channel, second row: N channel, third row: Z channel\n",
        "    data = np.array(dataset) # 6000x3\n",
        "\n",
        "    # convering hdf5 dataset into obspy sream\n",
        "    st = make_stream(dataset)\n",
        "\n",
        "\n",
        "    inventory = client.get_stations(network=dataset.attrs['network_code'],\n",
        "                                station=dataset.attrs['receiver_code'],\n",
        "                                starttime=UTCDateTime(dataset.attrs['trace_start_time']),\n",
        "                                endtime=UTCDateTime(dataset.attrs['trace_start_time']) + 60,\n",
        "                                loc=\"*\",\n",
        "                                channel=\"*\",\n",
        "                                level=\"response\")\n",
        "\n",
        "\n",
        "    # converting into displacement\n",
        "    st = make_stream(dataset)\n",
        "\n",
        "    try:\n",
        "        st = st.remove_response(inventory=inventory, output=\"ACC\", plot=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Stream-wide response removal failed: {e}\")\n",
        "        traces = np.full((3, 6000), np.nan, dtype=np.float64)  # Use float32 for compatibility  # Shape: (3, datapoints)\n",
        "        waveform_list.append(traces)\n",
        "        continue\n",
        "\n",
        "\n",
        "    st = st.remove_response(inventory=inventory, output=\"ACC\", plot=False)\n",
        "\n",
        "    # Convert ObsPy stream to NumPy array (shape: 3 x datapoints)\n",
        "    traces = np.array([tr.data for tr in st])  # Shape: (3, datapoints)\n",
        "\n",
        "    # Store in list\n",
        "    waveform_list.append(traces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following we check if it's possible to parallelize the process of conversion. If on your machine it's not possible we suggest to either run the notebook on google colab or to convert the loop into a single processor one (which should be straightforward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxSdMzi-DGkM",
        "outputId": "9f87f0bf-090a-448a-e00f-7bd0cf854f5f"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "print(multiprocessing.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2dPp2dWDWWn",
        "outputId": "4442d010-5645-437a-f2de-392ed75d85a9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(f\"Max threads: {os.cpu_count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX13zlgSD17o",
        "outputId": "e81b8765-8a26-44a3-f115-1e8c6fd7ea15"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def worker(n):\n",
        "    \"\"\"Simple function that simulates work by sleeping.\"\"\"\n",
        "    print(f\"Thread {threading.current_thread().name} is running\")\n",
        "    time.sleep(2)  # Simulates some work\n",
        "    return n\n",
        "\n",
        "# Set number of threads (e.g., 4)\n",
        "num_threads = 4\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    results = list(executor.map(worker, range(num_threads)))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# if it remains an average of 2 sec then he workers are all working and there is not a cap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPZEAwdY4Gx7"
      },
      "source": [
        "## Parallel Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gowsKn0ITfXs"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "from obspy import UTCDateTime\n",
        "from obspy.clients.fdsn import Client\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from multiprocessing import shared_memory\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVYCZT8-io4k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import time\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from multiprocessing import Array\n",
        "from obspy import UTCDateTime\n",
        "from obspy.clients.fdsn import Client\n",
        "from obspy.core.stream import Stream\n",
        "\n",
        "# Shared memory parameters\n",
        "num_channels = 3\n",
        "num_datapoints = 6000\n",
        "num_events = len(ev_list)  # Make sure ev_list is defined\n",
        "\n",
        "# Create a shared NumPy array using multiprocessing.Array\n",
        "shared_array_base = Array(\"d\", num_events * num_channels * num_datapoints)  # Shared flat array\n",
        "shared_array = np.frombuffer(shared_array_base.get_obj(), dtype=np.float64)\n",
        "shared_array = shared_array.reshape((num_events, num_channels, num_datapoints))\n",
        "shared_array[:] = np.nan  # Initialize with NaN\n",
        "\n",
        "client = Client(\"IRIS\")  # Persistent FDSN client\n",
        "\n",
        "def process_waveform(index_evi):\n",
        "    \"\"\"Function to process a single waveform event and write directly to shared memory.\"\"\"\n",
        "    index, evi = index_evi\n",
        "\n",
        "    with h5py.File(file_name, \"r\") as dtfl:\n",
        "        dataset = dtfl.get(f\"data/{evi}\")\n",
        "        if dataset is None:\n",
        "            return  # Skip if missing\n",
        "\n",
        "        try:\n",
        "            # Convert dataset into ObsPy stream (Assuming make_stream is defined elsewhere)\n",
        "            st = make_stream(dataset)\n",
        "\n",
        "            # Pre-fetch metadata\n",
        "            inv = client.get_stations(\n",
        "                network=dataset.attrs[\"network_code\"],\n",
        "                station=dataset.attrs[\"receiver_code\"],\n",
        "                starttime=UTCDateTime(dataset.attrs[\"trace_start_time\"]),\n",
        "                endtime=UTCDateTime(dataset.attrs[\"trace_start_time\"]) + 60,\n",
        "                loc=\"*\",\n",
        "                channel=\"*\",\n",
        "                level=\"response\"\n",
        "            )\n",
        "\n",
        "            # Remove response\n",
        "            st = st.remove_response(inventory=inv, output=\"ACC\", plot=False)\n",
        "\n",
        "            # Write results directly to shared array\n",
        "            shared_array[index] = np.array([tr.data for tr in st])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed processing event {evi}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ogYZwDSq2Q",
        "outputId": "ad35d0b3-8a70-417c-9ea0-a68d8faf953e"
      },
      "outputs": [],
      "source": [
        "# Use limited workers\n",
        "# num_workers = min(os.cpu_count(), len(ev_list))\n",
        "num_workers = 32\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "    executor.map(process_waveform, enumerate(ev_list))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Convert shared array into a regular NumPy array (optional)\n",
        "waveform_array = shared_array.copy()\n",
        "print(waveform_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNvCe25UTjUf",
        "outputId": "c087b659-0996-4675-8fb1-a5901719ffb5"
      },
      "outputs": [],
      "source": [
        "# Convert list to a PyTorch tensor of shape (n_samples, 3, datapoints)\n",
        "waveform_tensor = torch.tensor(np.stack(waveform_array), dtype=torch.float64)  # (n_samples, 3, datapoints)\n",
        "\n",
        "# Save the tensor to a .pt file\n",
        "torch.save(waveform_tensor, f\"path/to/save/{chunk}_acceleration.pt\")\n",
        "\n",
        "# Close HDF5 file\n",
        "#dtfl.close()\n",
        "\n",
        "print(f\"Tensor saved as {chunk}_acceleration.pt with shape:\", waveform_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82IZ8YJco2Rn"
      },
      "outputs": [],
      "source": [
        "# check to see it prints the right stuff\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "direction = 0\n",
        "\n",
        "for idx in range(10):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.plot(waveform_array[idx,direction,:], \"k-\")\n",
        "  plt.ylabel('acceleration')\n",
        "  plt.title('timesteps')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "EbHirA40pZLE",
        "outputId": "c3ab1ddf-7ae0-426a-fff9-2b6d725b677b"
      },
      "outputs": [],
      "source": [
        "# Convert list to a PyTorch tensor of shape (n_samples, 3, datapoints)\n",
        "waveform_tensor = torch.tensor(np.stack(waveform_array), dtype=torch.float64)  # (n_samples, 3, datapoints)\n",
        "\n",
        "# Save the tensor to a .pt file\n",
        "torch.save(waveform_tensor, f\"STEAD_data/{chunk}/{chunk}_acceleration.pt\")\n",
        "\n",
        "# Close HDF5 file\n",
        "dtfl.close()\n",
        "\n",
        "print(f\"Tensor saved as {chunk}_acceleration.pt with shape:\", waveform_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKQ4joHq2wDG",
        "outputId": "2a432bea-2e22-4cac-d960-d5d0ab4b086a"
      },
      "outputs": [],
      "source": [
        "accelerations = torch.load(f\"STEAD_data/{chunk}/{chunk}_acceleration.pt\")\n",
        "print(accelerations.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "i5-z46Nu3_SE",
        "outputId": "fce39530-fc3a-48cf-8d48-79db896dcb81"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = 1\n",
        "direction = 0\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(accelerations[idx,direction,:], \"k-\")\n",
        "plt.ylabel('acceleration')\n",
        "plt.title('timesteps')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GgRe_aBE2Zka",
        "RrV3s-xb0iGg",
        "0UMtkGMnhYcE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
